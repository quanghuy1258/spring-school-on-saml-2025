{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7843718f",
   "metadata": {},
   "source": [
    "## Introduction to random matrices: practical session ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd8be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8286c48",
   "metadata": {},
   "source": [
    "**Basic matrix and random variables manipulations in Python.** Don't hesitate to check the documentarion pages of numpy and scipy, for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8b441",
   "metadata": {},
   "source": [
    "Experimentation is strongly encouraged!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b015d24",
   "metadata": {},
   "source": [
    "You can create matrices with the `array` command of numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344df7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Matrix A:\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dae4e",
   "metadata": {},
   "source": [
    "The transpose of a matrix is obtained via `np.transpose` or `.T`. Print the transpose of the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e94aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e3831",
   "metadata": {},
   "source": [
    "To extract the upper triangular part and the diagonal part of a matrix, you can use `np.triu` and `np.diag` respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34aca9",
   "metadata": {},
   "source": [
    "Print the upper triangular and the diagonal part of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f7bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be1247",
   "metadata": {},
   "source": [
    "Replace the diagonal part of $A$ by $[10,11,12]$ using `np.fill_diagonal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22259f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0551f",
   "metadata": {},
   "source": [
    "Basic operations on matrices such as addition, soustraction and multiplication by a scalar are done by `+`, `-`, and `*` respectively, and multiplication of two matrices $A$ and $B$ is done by `np.dot(A,B)` or `A @ B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b726c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix B:\n",
      " [[-1  0  2]\n",
      " [ 3  1  4]\n",
      " [ 1  0 -1]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[-1, 0, 2], [3, 1, 4], [1, 0, -1]])\n",
    "print(\"Matrix B:\\n\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872a1eb",
   "metadata": {},
   "source": [
    "Print $A+B$, $A-B$, $2A$, $AB$ and $BA$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299355e",
   "metadata": {},
   "source": [
    "We can generate random samples using the `random` submodule of numpy, so for instance, the following command will provide a sample of 10 independant random variables with $\\mathcal N(0,1)$ distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa23307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.40106009, -0.45532507,  1.66989388, -0.66961265,  0.09779903,\n",
       "       -1.04724648,  0.44313069,  0.74926691, -0.79308834, -0.98645059])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0,1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89624936",
   "metadata": {},
   "source": [
    "Classical distribution such as the Binomial distribution or the uniform distribution are given by `np.random.binomial`, `np.random.uniform`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404698fd",
   "metadata": {},
   "source": [
    "We can also generate an array of i.i.d. random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f436f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.67775112, -0.97894871, -0.04927008],\n",
       "       [-0.98859687, -0.1089321 , -0.49349768],\n",
       "       [-0.04384271,  0.72679017,  0.63764611]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0,1,(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716d698",
   "metadata": {},
   "source": [
    "Generate a $5 \\times  5$ matrix with i.i.d. symmetric Bernoulli random variables on $\\{-1,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e791db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cddab7",
   "metadata": {},
   "source": [
    "Eigenvalues and Eigenvectors of a matrix are obtained by the function `np.linalg.eig`. We can just use the function `np.linalg.eigvals`to compute the eigenvalues. Moreover, for Hermitian or symmetric matrices the function `np.linalg.eigh` performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb8fbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.61168440e+01, -1.11684397e+00, -1.30367773e-15]),\n",
       " array([[-0.23197069, -0.78583024,  0.40824829],\n",
       "        [-0.52532209, -0.08675134, -0.81649658],\n",
       "        [-0.8186735 ,  0.61232756,  0.40824829]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f946c9b",
   "metadata": {},
   "source": [
    "Generate a $5\\times 5$ symmetric random matrix with i.i.d. (up to the symmetry) coefficients distributed according your favorite distribution, and compare the eigenvalues computed by `np.linalg.eig` and `np.linalg.eigh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba350812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fa616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6dd86ec",
   "metadata": {},
   "source": [
    "### Wigner's Theorem and the semi-circular law ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25e8b0",
   "metadata": {},
   "source": [
    "Illustrate the Wigner's theorem: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857a0a3",
   "metadata": {},
   "source": [
    "Generate a $n \\times n$ symmetric random matrix $X$ with i.i.d. real Gaussian coefficients (up to the symmetry) with mean zero and variance one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051fcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69073a",
   "metadata": {},
   "source": [
    "Define the density of the semi-circular distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a5e404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917442b",
   "metadata": {},
   "source": [
    "Plot the histogram of $\\frac{1}{\\sqrt n} X$ together with the density of the semi-circular distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3a7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9feb3",
   "metadata": {},
   "source": [
    "Do the same with a Hermitian random matrix with i.i.d. complex coefficients with mean zero and variance one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d598cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbaca3",
   "metadata": {},
   "source": [
    "Illustrate the universality of Wigner's theorem by changing the distribution of the coefficients of $X$ (for instance uniform distribution on $[-1,1]$, Bernoulli distribution on $\\{-1,1\\}$, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d417246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e0583",
   "metadata": {},
   "source": [
    "Consider now the Student's t distribution. The Student's t distribution with parameter $c$ is the probability measure with density defined by:\n",
    "$$\n",
    "\\frac{\\Gamma(\\frac{c+1}{2})}{\\sqrt{\\pi c} \\Gamma(\\frac{c}{2})} \\left(1 + \\frac{x^2}{c}   \\right)^{-(c+1)/2}\n",
    "$$\n",
    "Plot the histogram of the empirical spectral distribution of $\\frac{1}{\\sqrt n} X$ with coefficient distributed according to the Student distribution with parameter 2 (use `np.random.standard_t`). What is happening here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e57b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19240424",
   "metadata": {},
   "source": [
    "### A few hand computations! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1d36d",
   "metadata": {},
   "source": [
    "To check your understanding of the proof of Wigner's Theorem by the method of moments, try the following exercise: give a proof of the Central Limit Theorem by the method of moments! (this is much easier than Wigner's Theorem) \n",
    "\n",
    "Let $(X_k)_{k\\geq1}$ be a sequence of i.i.d. random variables, with bounded moments of any order, and such that $\\mathbb E(X_1)=0$ and $\\mathbb E(X_1^2)=1$. We recall that the double factorial at odd integers is defined by:\n",
    "$$\n",
    "(2k-1)!!=(2k-1)(2k-3)\\cdots 3 \\cdot1= \\frac{(2k)!}{2^k k!}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85af06",
   "metadata": {},
   "source": [
    "1) Show that the moments of the Gaussian distribution are given by:\n",
    "$$\n",
    "\\mathbb E \\left( N^{2k}   \\right) = (2k-1)!!, \\quad \\mathbb E \\left( N^{2k+1}   \\right)=0,\n",
    "$$\n",
    "where $N$ is distributed according to the $\\mathcal N(0,1)$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58c481",
   "metadata": {},
   "source": [
    "2) Show that the number of pairings of $\\{1,\\ldots,2k\\}$ is $(2k-1)!!$, where a pairing of $\\{1,\\ldots,2k\\}$ is a partition $\\{V_1,\\ldots,V_k\\}$ such that $|V_i|=2$ for any $i\\in \\{1,\\ldots,k\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595cdeb",
   "metadata": {},
   "source": [
    "3) Expand\n",
    "$$\n",
    "\\mathbb E \\left[ \\left( \\frac{X_1+\\cdots+X_n}{\\sqrt n} \\right)^k \\right]\n",
    "$$\n",
    "and show that, as $n\\to\\infty$, the only terms in the expansion that gives a non-zero contribution are those for which each $X_i$ appears exactly twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53cb4c",
   "metadata": {},
   "source": [
    "4) Show that for all $k \\geq 0$,\n",
    "$$\n",
    "\\mathbb E \\left[ \\left( \\frac{X_1+\\cdots+X_n}{\\sqrt n} \\right)^k \\right] \\to \\mathbb E \\left( N^k  \\right),\n",
    "$$\n",
    "as $n\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185eee6",
   "metadata": {},
   "source": [
    "5) The Gaussian distribution has not a compact support, but is still characterized by its moments (see for instance, Billingsley, Probability and measure, chapter 30). Conclude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba94cef",
   "metadata": {},
   "source": [
    "### Sample covariance matrices and the Marchenko-Pastur distribution ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24971abf",
   "metadata": {},
   "source": [
    "Now consider a $p\\times n$ random matrix with i.i.d. $\\mathcal N(0,1)$ coefficients. Plot the histogram of the eigenvalues of $\\frac{1}{n} X X^\\intercal$ together with the density of the Marchenko-Pastur distribution. Try different values of $p$ and $n$ (so $\\frac{p}{n}$ is greater or less than 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f605c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31175583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54d07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb83bc41",
   "metadata": {},
   "source": [
    "### Concentration of Gaussian vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ad9db",
   "metadata": {},
   "source": [
    "Consider a Gaussian vector $X \\in \\mathbb R^p$ with covariance matrix the identity. Plot the histogram of $f(X)$, for the 1-norm $||\\cdot||_1$, the $2$-norm $||\\cdot||_2$ and the supremum norm $||\\cdot||_\\infty$, for 1000 realisations of $X$, for $p=1,10,100,1000$. You can use `np.linalg.norm(..., ord=k)` for the $k$-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9f3ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa74c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc54d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6270a526",
   "metadata": {},
   "source": [
    "### Fluctuations of the largest eigenvalue ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f93af8",
   "metadata": {},
   "source": [
    "For both Wigner matrices and sample covariance matrices, the largest eigenvalue converges to the edge of the bulk. One can show that the fluctuations are governed by the Tracy-Widom distribution. More precisely: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687980f",
   "metadata": {},
   "source": [
    "Generate $N=500$ GOE matrices, and plot the histogram of the largest eigenvalue (note that it can be a bit long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "106f3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb85b917",
   "metadata": {},
   "source": [
    "In fact, the fluctuations of the largest eigenvalue of the GOE ensemble is governed by the so-called Tracy-Widom distribution: for all $t\\in \\mathbb R$,\n",
    "$$\n",
    "\\lim_{n\\to\\infty}\\mathbb P\\left(n^{1/6}(\\lambda_\\max - 2 \\sqrt n) \\leq t   \\right) = F_{1}(t),\n",
    "$$\n",
    "where $F_{1}$ is the cumulative distribution function of the Tracy-Widom distribution (for the GOE case). This distribution is quite complicated to define (it is related to Painlevé equation). The following code gives an approximation of the density of the Tracy-Widom distribution (thanks to Zhenyu Liao for providing the code). $F_1$ corresponds to the GOE ensemble, $F_2$ to the GUE ensemble, and $F_4$ to the GSE ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce304e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "def tracy_widom_appx(x, i):\n",
    "#\n",
    "# TW ~ Gamma[k,theta]-alpha\n",
    "#\n",
    "# [pdf,cdf]=tracywidom_appx(x,i) for i=1,2,4 gives TW1, TW2, TW4\n",
    "#\n",
    "\n",
    "    kappx = [46.44604884387787, 79.6594870666346, 0, 146.0206131050228]   #  K, THETA, ALPHA\n",
    "    thetaappx = [0.18605402228279347, 0.10103655775856243, 0, 0.05954454047933292]\n",
    "    alphaappx = [9.848007781128567, 9.819607173436484, 0, 11.00161520109004]\n",
    "\n",
    "    cdftwappx = cdfgamma(x+alphaappx[i-1], thetaappx[i-1], kappx[i-1])\n",
    "\n",
    "    pdftwappx = pdfgamma(x+alphaappx[i-1], thetaappx[i-1], kappx[i-1])\n",
    "\n",
    "    return pdftwappx, cdftwappx\n",
    "\n",
    "# Probability density function\n",
    "\n",
    "def pdfgamma(x, ta, ka):\n",
    "    pdf = []\n",
    "    for x_ in x:\n",
    "        if x_ > 0:\n",
    "            pdf.append(1/(scipy.special.gamma(ka)*ta**ka) * x_**(ka - 1) * np.exp(-x_/ta))\n",
    "        else:\n",
    "            pdf.append(0)\n",
    "    return pdf\n",
    "\n",
    "# Cumulative density function\n",
    "\n",
    "def cdfgamma(x, ta, ka):\n",
    "    cdf = []\n",
    "    for x_ in x:\n",
    "        if x_ > 0:\n",
    "            cdf.append(scipy.special.gammainc(ka,x_/ta))\n",
    "        else:\n",
    "            cdf.append(0)\n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515242d5",
   "metadata": {},
   "source": [
    "Note that the normalization in $n^{1/6}$ can be heuristically understand as follows: by the Wigner's theorem, one has\n",
    "$$\n",
    "n \\mu_n \\left( [2-\\varepsilon,2]  \\right) \\approx n \\int_{2-\\varepsilon}^2 \\sqrt{4-x^2} dx \\approx n \\varepsilon^{2/3},\n",
    "$$\n",
    "so $\\varepsilon \\sim n^{-2/3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd5211",
   "metadata": {},
   "source": [
    "Plot the histogram of the largest eigenvalue of the GOE ensemble and compare to the Tracy-Widom distribution $F_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e65c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39764de",
   "metadata": {},
   "source": [
    "Try the same with a Wishart matrix. Hint: first try to understand the renormalization as for Wigner's matrices and compare to the Tracy-Widom distribution. Try the write out a statement about the fluctuations of the largest eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a4596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2573f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eede3eb3",
   "metadata": {},
   "source": [
    "### Deformed models ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd2120",
   "metadata": {},
   "source": [
    "We now consider deformed models of finite rank deterministic matrices by a random matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b1a2a",
   "metadata": {},
   "source": [
    "We start with the additive case: let \n",
    "$$\n",
    "M_n = \\frac{1}{\\sqrt n} X_n + A_n,\n",
    "$$\n",
    "where $X_n$ is a Wigner matrix, with, for instance, $\\mathcal N(0,1)$ distribution, and $A_n$ a finite rank diagonal deterministic matrix. Start with a rank-one matrix $A_n$, so $A_n= \\theta e_1e_1^\\intercal$, where $e_1$ is the first vector of the canonical basis of $\\mathbb R^n$. \n",
    "Plot the histogram of the eigenvalues of $M_n$ and compare to the semi-circular distribution. Try different values of $\\theta$. An outlier will arise if and only if $\\theta>1$, the outlier being $\\theta + \\frac{1}{\\theta}$ (which is striclty greater than $2$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8dd1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c0d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc19329b",
   "metadata": {},
   "source": [
    "Do the same with a finite rank matrix with several spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "700b3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f9577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b7c98a",
   "metadata": {},
   "source": [
    "Now, perform the case of sample covariance matrices. Consider the model\n",
    "$$\n",
    "M_n = \\frac{1}{n} YY^\\intercal,\n",
    "$$\n",
    "where $Y=\\Sigma^{1/2} X$, with $X$ a $p\\times n$ Gaussian random matrix, and $\\Sigma$ a $p \\times p$ deterministic diagonal matrix, with finite rank. Start with the simplest example, where $\\Sigma_{1,1}=\\theta$, and $\\Sigma_{i,i}=1$, for all $i\\geq2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0359e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183914f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3682c14a",
   "metadata": {},
   "source": [
    "#### Population covariance estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862fc75",
   "metadata": {},
   "source": [
    "Define for example, a diagonal matrix $\\Sigma$ with $3$ different eigenvalues, for example $1,3,7$, with equal proportion. Plot the histogram of empirical eigenvalue distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63a3b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f73c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f032323f",
   "metadata": {},
   "source": [
    "Recall that by Bai and Silverstein Theorem, the Stieltjes transforms $m$ and $\\tilde m$ of the limiting spectral distribution of the model $\\frac{1}{n} \\Sigma^{1/2} X X^\\intercal \\Sigma^{1/2}$ and $\\frac{1}{n} X^\\intercal \\Sigma X$  satisfy the fixed point equation on $\\mathbb C \\setminus \\mathbb R$:\n",
    "$$\n",
    "m(z) = \\frac{1}{c}\\tilde m(z) + \\frac{1-c}{c z},   \\quad\\tilde m(z)= \\left(- z + c\\int \\frac{t}{1+t \\tilde m(z)} \\nu(dt)   \\right)^{-1},\n",
    "$$\n",
    "where $\\nu$ is the limiting spectral distribution of $\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f05af3",
   "metadata": {},
   "source": [
    "We can plot an approximation of the density of $m$ using a standard fixed point algorithm (one can show that such an algorithm converges). One has $m(z)=\\lim_{l\\to\\infty} m^{(l)}(z)$, where we start by say $\\tilde m(z)=0$, and for all $l\\geq1$,\n",
    "$$\n",
    "m^{(l)}(z) = \\frac{1}{c}\\tilde m^{(l)}(z) + \\frac{1-c}{c z},   \\quad\\tilde m^{(l+1)}(z)= \\left(- z + c\\int \\frac{t}{1+t \\tilde m^(l)(z)} \\nu(dt)   \\right)^{-1}.\n",
    "$$\n",
    "One should be careful since $m(z)$ is not defined for $z\\in \\text{supp} (\\mu)$. So one has to perform the algorithm for $z$ close to the real axis, that is $z=x+i\\varepsilon$, with $\\varepsilon \\ll 1$, (say $10^{-5}$), but the convergence can be quite slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f68d8b",
   "metadata": {},
   "source": [
    "Plot an approximation of the density of the distribution characterized by $m$ for the above example of $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "147e9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5eade",
   "metadata": {},
   "source": [
    "Compare with the histogram of the eigenvalues of $\\frac{1}{n} \\Sigma^{1/2} X X^\\intercal \\Sigma^{1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a48f94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7fe55",
   "metadata": {},
   "source": [
    "####  Population eigenvalue estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebf428",
   "metadata": {},
   "source": [
    "In view of the plot, it may be appear that averaging the sample eigenvalues of each component of the empirical spectral distribution provide a consistent estimator. But it is not the case, and such an estimator is indeed biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8888a",
   "metadata": {},
   "source": [
    "Visualization of the local behavior of the Stieltjes transform around an eigvanlue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220351da",
   "metadata": {},
   "source": [
    "Assume $p<n$. The Stieltjes transform of the sample covariance $n\\times n$ matrix $\\frac1n Y^\\intercal Y$, with $Y= \\Sigma^{1/2} X$ is\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{k=1}^p \\frac{1}{\\lambda_k-z} - \\frac{n-p}{n} \\frac{1}{z} \n",
    "$$\n",
    "where $\\lambda_1 \\leq \\cdots \\leq \\lambda_p$ are the sorted eigenvalues of $\\frac1n Y Y^{\\intercal}$ (and there are $n-p$ eigenvalues of $\\frac1n Y^\\intercal Y$ equal to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85ace1",
   "metadata": {},
   "source": [
    "It can be proved that the zeroes of the Stieltjes transform of the sample covariance $n\\times n$ matrix $\\frac1n Y^\\intercal Y$ are given by the eigenvalues of the following matrix:\n",
    "$$\n",
    "\\Lambda - \\frac{1}{n}\\sqrt \\lambda \\sqrt \\lambda^\\intercal,\n",
    "$$\n",
    "where $\\Lambda$ is the diagonal $p\\times p$ matrix $\\text{diag}(\\lambda_1,\\ldots,\\lambda_p)$, and $\\lambda$ the column vector $(\\lambda_1,\\ldots,\\lambda_p)$ in $\\mathbb R^p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c93c04",
   "metadata": {},
   "source": [
    "Plot the Stieltjes transform of the sample covariance matrix $\\frac1n  Y^\\intercal Y$, with $Y= \\Sigma^{1/2} X$ near one eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce1175ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e6b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662aef17",
   "metadata": {},
   "source": [
    "Now, Mestre has proved that for $\\Sigma$ with eigenvalues $0<l_1<\\ldots<l_k$ with multiplicities $N_1,\\ldots,N_k$ (so $\\sum_{j=1}^k N_j = p$), the following gives a consistent estimator for $l_a$:\n",
    "$$\n",
    "\\hat l_a = \\frac{n}{N_a} \\sum_{j=N_1+\\cdots+N_{a-1}+1}^{N_1+\\cdots+N_{a}} \\left( \\lambda_j - \\rho_j  \\right),\n",
    "$$\n",
    "where $\\rho_1,\\ldots,\\rho_p$ are the zeroes of the Stieltjes transform of $\\frac1n Y^\\intercal Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4999b0a",
   "metadata": {},
   "source": [
    "For the above example for a population of 3 eigenvalues, compare the performance of the naive estimator of $l_a$ obtained by averaging the sample eigenvalues of each component of the empirical spectral distribution, and the above improved estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d98b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fd20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976069e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
