{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7843718f",
   "metadata": {},
   "source": [
    "## Introduction to random matrices: practical session ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd8be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8286c48",
   "metadata": {},
   "source": [
    "**Basic matrix and random variables manipulations in Python.** Don't hesitate to check the documentarion pages of numpy and scipy, for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8b441",
   "metadata": {},
   "source": [
    "Experimentation is strongly encouraged!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b015d24",
   "metadata": {},
   "source": [
    "You can create matrices with the `array` command of numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344df7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Matrix A:\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dae4e",
   "metadata": {},
   "source": [
    "The transpose of a matrix is obtained via `np.transpose` or `.T`. Print the transpose of the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e94aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e3831",
   "metadata": {},
   "source": [
    "To extract the upper triangular part and the diagonal part of a matrix, you can use `np.triu` and `np.diag` respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34aca9",
   "metadata": {},
   "source": [
    "Print the upper triangular and the diagonal part of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f7bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be1247",
   "metadata": {},
   "source": [
    "Replace the diagonal part of $A$ by $[10,11,12]$ using `np.fill_diagonal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22259f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0551f",
   "metadata": {},
   "source": [
    "Basic operations on matrices such as addition, soustraction and multiplication by a scalar are done by `+`, `-`, and `*` respectively, and multiplication of two matrices $A$ and $B$ is done by `np.dot(A,B)` or `A @ B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b726c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix B:\n",
      " [[-1  0  2]\n",
      " [ 3  1  4]\n",
      " [ 1  0 -1]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[-1, 0, 2], [3, 1, 4], [1, 0, -1]])\n",
    "print(\"Matrix B:\\n\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872a1eb",
   "metadata": {},
   "source": [
    "Print $A+B$, $A-B$, $2A$, $AB$ and $BA$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71920f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299355e",
   "metadata": {},
   "source": [
    "We can generate random samples using the `random` submodule of numpy, so for instance, the following command will provide a sample of 10 independant random variables with $\\mathcal N(0,1)$ distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa23307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.40106009, -0.45532507,  1.66989388, -0.66961265,  0.09779903,\n",
       "       -1.04724648,  0.44313069,  0.74926691, -0.79308834, -0.98645059])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0,1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89624936",
   "metadata": {},
   "source": [
    "Classical distribution such as the Binomial distribution or the uniform distribution are given by `np.random.binomial`, `np.random.uniform`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404698fd",
   "metadata": {},
   "source": [
    "We can also generate an array of i.i.d. random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f436f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.67775112, -0.97894871, -0.04927008],\n",
       "       [-0.98859687, -0.1089321 , -0.49349768],\n",
       "       [-0.04384271,  0.72679017,  0.63764611]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0,1,(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716d698",
   "metadata": {},
   "source": [
    "Generate a $5 \\times  5$ matrix with i.i.d. symmetric Bernoulli random variables on $\\{-1,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e791db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cddab7",
   "metadata": {},
   "source": [
    "Eigenvalues and Eigenvectors of a matrix are obtained by the function `np.linalg.eig`. We can just use the function `np.linalg.eigvals`to compute the eigenvalues. Moreover, for Hermitian or symmetric matrices the function `np.linalg.eigh` performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb8fbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.61168440e+01, -1.11684397e+00, -1.30367773e-15]),\n",
       " array([[-0.23197069, -0.78583024,  0.40824829],\n",
       "        [-0.52532209, -0.08675134, -0.81649658],\n",
       "        [-0.8186735 ,  0.61232756,  0.40824829]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f946c9b",
   "metadata": {},
   "source": [
    "Generate a $5\\times 5$ symmetric random matrix with i.i.d. (up to the symmetry) coefficients distributed according your favorite distribution, and compare the eigenvalues computed by `np.linalg.eig` and `np.linalg.eigh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba350812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fa616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6dd86ec",
   "metadata": {},
   "source": [
    "### Wigner's Theorem and the semi-circular law ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25e8b0",
   "metadata": {},
   "source": [
    "Illustrate the Wigner's theorem: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857a0a3",
   "metadata": {},
   "source": [
    "Generate a $n \\times n$ symmetric random matrix $X$ with i.i.d. real Gaussian coefficients (up to the symmetry) with mean zero and variance one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051fcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69073a",
   "metadata": {},
   "source": [
    "Define the density of the semi-circular distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a5e404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917442b",
   "metadata": {},
   "source": [
    "Plot the histogram of $\\frac{1}{\\sqrt n} X$ together with the density of the semi-circular distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3a7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf9feb3",
   "metadata": {},
   "source": [
    "Do the same with a Hermitian random matrix with i.i.d. complex coefficients with mean zero and variance one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d598cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbaca3",
   "metadata": {},
   "source": [
    "Illustrate the universality of Wigner's theorem by changing the distribution of the coefficients of $X$ (for instance uniform distribution on $[-1,1]$, Bernoulli distribution on $\\{-1,1\\}$, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d417246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e0583",
   "metadata": {},
   "source": [
    "Consider now the Student's t distribution. The Student's t distribution with parameter $c$ is the probability measure with density defined by:\n",
    "$$\n",
    "\\frac{\\Gamma(\\frac{c+1}{2})}{\\sqrt{\\pi c} \\Gamma(\\frac{c}{2})} \\left(1 + \\frac{x^2}{c}   \\right)^{-(c+1)/2}\n",
    "$$\n",
    "Plot the histogram of the empirical spectral distribution of $\\frac{1}{\\sqrt n} X$ with coefficient distributed according to the Student distribution with parameter 2 (use `np.random.standard_t`). What is happening here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e57b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19240424",
   "metadata": {},
   "source": [
    "### A few hand computations! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1d36d",
   "metadata": {},
   "source": [
    "To check your understanding of the proof of Wigner's Theorem by the method of moments, try the following exercise: give a proof of the Central Limit Theorem by the method of moments! (this is much easier than Wigner's Theorem) \n",
    "\n",
    "Let $(X_k)_{k\\geq1}$ be a sequence of i.i.d. random variables, with bounded moments of any order, and such that $\\mathbb E(X_1)=0$ and $\\mathbb E(X_1^2)=1$. We recall that the double factorial at odd integers is defined by:\n",
    "$$\n",
    "(2k-1)!!=(2k-1)(2k-3)\\cdots 3 \\cdot1= \\frac{(2k)!}{2^k k!}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85af06",
   "metadata": {},
   "source": [
    "1) Show that the moments of the Gaussian distribution are given by:\n",
    "$$\n",
    "\\mathbb E \\left( N^{2k}   \\right) = (2k-1)!!, \\quad \\mathbb E \\left( N^{2k+1}   \\right)=0,\n",
    "$$\n",
    "where $N$ is distributed according to the $\\mathcal N(0,1)$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58c481",
   "metadata": {},
   "source": [
    "2) Show that the number of pairings of $\\{1,\\ldots,2k\\}$ is $(2k-1)!!$, where a pairing of $\\{1,\\ldots,2k\\}$ is a partition $\\{V_1,\\ldots,V_k\\}$ such that $|V_i|=2$ for any $i\\in \\{1,\\ldots,k\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595cdeb",
   "metadata": {},
   "source": [
    "3) Expand\n",
    "$$\n",
    "\\mathbb E \\left[ \\left( \\frac{X_1+\\cdots+X_n}{\\sqrt n} \\right)^k \\right]\n",
    "$$\n",
    "and show that, as $n\\to\\infty$, the only terms in the expansion that gives a non-zero contribution are those for which each $X_i$ appears exactly twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53cb4c",
   "metadata": {},
   "source": [
    "4) Show that for all $k \\geq 0$,\n",
    "$$\n",
    "\\mathbb E \\left[ \\left( \\frac{X_1+\\cdots+X_n}{\\sqrt n} \\right)^k \\right] \\to \\mathbb E \\left( N^k  \\right),\n",
    "$$\n",
    "as $n\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185eee6",
   "metadata": {},
   "source": [
    "5) The Gaussian distribution has not a compact support, but is still characterized by its moments (see for instance, Billingsley, Probability and measure, chapter 30). Conclude. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba94cef",
   "metadata": {},
   "source": [
    "### Sample covariance matrices and the Marchenko-Pastur distribution ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24971abf",
   "metadata": {},
   "source": [
    "Now consider a $p\\times n$ random matrix with i.i.d. $\\mathcal N(0,1)$ coefficients. Plot the histogram of the eigenvalues of $\\frac{1}{n} X X^\\intercal$ together with the density of the Marchenko-Pastur distribution. Try different values of $p$ and $n$ (so $\\frac{p}{n}$ is greater or less than 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f605c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31175583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa54d07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb83bc41",
   "metadata": {},
   "source": [
    "### Concentration of Gaussian vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ad9db",
   "metadata": {},
   "source": [
    "Consider a Gaussian vector $X \\in \\mathbb R^p$ with covariance matrix the identity. Plot the histogram of $f(X)$, for the 1-norm $||\\cdot||_1$, the $2$-norm $||\\cdot||_2$ and the supremum norm $||\\cdot||_\\infty$, for 1000 realisations of $X$, for $p=1,10,100,1000$. You can use `np.linalg.norm(..., ord=k)` for the $k$-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9f3ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa74c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc54d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6270a526",
   "metadata": {},
   "source": [
    "### Fluctuations of the largest eigenvalue ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f93af8",
   "metadata": {},
   "source": [
    "For both Wigner matrices and sample covariance matrices, the largest eigenvalue converges to the edge of the bulk. One can show that the fluctuations are governed by the Tracy-Widom distribution. More precisely: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687980f",
   "metadata": {},
   "source": [
    "Generate $N=500$ GOE matrices, and plot the histogram of the largest eigenvalue (note that it can be a bit long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "106f3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb85b917",
   "metadata": {},
   "source": [
    "In fact, the fluctuations of the largest eigenvalue of the GOE ensemble is governed by the so-called Tracy-Widom distribution: for all $t\\in \\mathbb R$,\n",
    "$$\n",
    "\\lim_{n\\to\\infty}\\mathbb P\\left(n^{1/6}(\\lambda_\\max - 2 \\sqrt n) \\leq t   \\right) = F_{1}(t),\n",
    "$$\n",
    "where $F_{1}$ is the cumulative distribution function of the Tracy-Widom distribution (for the GOE case). This distribution is quite complicated to define (it is related to PainlevÃ© equation). The following code gives an approximation of the density of the Tracy-Widom distribution (thanks to Zhenyu Liao for providing the code). $F_1$ corresponds to the GOE ensemble, $F_2$ to the GUE ensemble, and $F_4$ to the GSE ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce304e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "def tracy_widom_appx(x, i):\n",
    "#\n",
    "# TW ~ Gamma[k,theta]-alpha\n",
    "#\n",
    "# [pdf,cdf]=tracywidom_appx(x,i) for i=1,2,4 gives TW1, TW2, TW4\n",
    "#\n",
    "\n",
    "    kappx = [46.44604884387787, 79.6594870666346, 0, 146.0206131050228]   #  K, THETA, ALPHA\n",
    "    thetaappx = [0.18605402228279347, 0.10103655775856243, 0, 0.05954454047933292]\n",
    "    alphaappx = [9.848007781128567, 9.819607173436484, 0, 11.00161520109004]\n",
    "\n",
    "    cdftwappx = cdfgamma(x+alphaappx[i-1], thetaappx[i-1], kappx[i-1])\n",
    "\n",
    "    pdftwappx = pdfgamma(x+alphaappx[i-1], thetaappx[i-1], kappx[i-1])\n",
    "\n",
    "    return pdftwappx, cdftwappx\n",
    "\n",
    "# Probability density function\n",
    "\n",
    "def pdfgamma(x, ta, ka):\n",
    "    pdf = []\n",
    "    for x_ in x:\n",
    "        if x_ > 0:\n",
    "            pdf.append(1/(scipy.special.gamma(ka)*ta**ka) * x_**(ka - 1) * np.exp(-x_/ta))\n",
    "        else:\n",
    "            pdf.append(0)\n",
    "    return pdf\n",
    "\n",
    "# Cumulative density function\n",
    "\n",
    "def cdfgamma(x, ta, ka):\n",
    "    cdf = []\n",
    "    for x_ in x:\n",
    "        if x_ > 0:\n",
    "            cdf.append(scipy.special.gammainc(ka,x_/ta))\n",
    "        else:\n",
    "            cdf.append(0)\n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515242d5",
   "metadata": {},
   "source": [
    "Note that the normalization in $n^{1/6}$ can be heuristically understand as follows: by the Wigner's theorem, one has\n",
    "$$\n",
    "n \\mu_n \\left( [2-\\varepsilon,2]  \\right) \\approx n \\int_{2-\\varepsilon}^2 \\sqrt{4-x^2} dx \\approx n \\varepsilon^{2/3},\n",
    "$$\n",
    "so $\\varepsilon \\sim n^{-2/3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd5211",
   "metadata": {},
   "source": [
    "Plot the histogram of the largest eigenvalue of the GOE ensemble and compare to the Tracy-Widom distribution $F_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e65c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39764de",
   "metadata": {},
   "source": [
    "Try the same with a Wishart matrix. Hint: first try to understand the renormalization as for Wigner's matrices and compare to the Tracy-Widom distribution. Try the write out a statement about the fluctuations of the largest eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a4596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2573f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eede3eb3",
   "metadata": {},
   "source": [
    "### Deformed models ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd2120",
   "metadata": {},
   "source": [
    "We now consider deformed models of finite rank deterministic matrices by a random matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b1a2a",
   "metadata": {},
   "source": [
    "We start with the additive case: let \n",
    "$$\n",
    "M_n = \\frac{1}{\\sqrt n} X_n + A_n,\n",
    "$$\n",
    "where $X_n$ is a Wigner matrix, with, for instance, $\\mathcal N(0,1)$ distribution, and $A_n$ a finite rank diagonal deterministic matrix. Start with a rank-one matrix $A_n$, so $A_n= \\theta e_1e_1^\\intercal$, where $e_1$ is the first vector of the canonical basis of $\\mathbb R^n$. \n",
    "Plot the histogram of the eigenvalues of $M_n$ and compare to the semi-circular distribution. Try different values of $\\theta$. An outlier will arise if and only if $\\theta>1$, the outlier being $\\theta + \\frac{1}{\\theta}$ (which is striclty greater than $2$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8dd1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c0d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc19329b",
   "metadata": {},
   "source": [
    "Do the same with a finite rank matrix with several spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "700b3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f9577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b7c98a",
   "metadata": {},
   "source": [
    "Now, perform the case of sample covariance matrices. Consider the model\n",
    "$$\n",
    "M_n = \\frac{1}{n} YY^\\intercal,\n",
    "$$\n",
    "where $Y=\\Sigma^{1/2} X$, with $X$ a $p\\times n$ Gaussian random matrix, and $\\Sigma$ a $p \\times p$ deterministic diagonal matrix, with finite rank. Start with the simplest example, where $\\Sigma_{1,1}=\\theta$, and $\\Sigma_{i,i}=1$, for all $i\\geq2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0359e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183914f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3682c14a",
   "metadata": {},
   "source": [
    "#### Population covariance estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862fc75",
   "metadata": {},
   "source": [
    "Define for example, a diagonal matrix $\\Sigma$ with $3$ different eigenvalues, for example $1,3,7$, with equal proportion. Plot the histogram of empirical eigenvalue distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63a3b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f73c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f032323f",
   "metadata": {},
   "source": [
    "Recall that by Bai and Silverstein Theorem, the Stieltjes transforms $m$ and $\\tilde m$ of the limiting spectral distribution of the model $\\frac{1}{n} \\Sigma^{1/2} X X^\\intercal \\Sigma^{1/2}$ and $\\frac{1}{n} X^\\intercal \\Sigma X$  satisfy the fixed point equation on $\\mathbb C \\setminus \\mathbb R$:\n",
    "$$\n",
    "m(z) = \\frac{1}{c}\\tilde m(z) + \\frac{1-c}{c z},   \\quad\\tilde m(z)= \\left(- z + c\\int \\frac{t}{1+t \\tilde m(z)} \\nu(dt)   \\right)^{-1},\n",
    "$$\n",
    "where $\\nu$ is the limiting spectral distribution of $\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f05af3",
   "metadata": {},
   "source": [
    "We can plot an approximation of the density of $m$ using a standard fixed point algorithm (one can show that such an algorithm converges). One has $m(z)=\\lim_{l\\to\\infty} m^{(l)}(z)$, where we start by say $\\tilde m(z)=0$, and for all $l\\geq1$,\n",
    "$$\n",
    "m^{(l)}(z) = \\frac{1}{c}\\tilde m^{(l)}(z) + \\frac{1-c}{c z},   \\quad\\tilde m^{(l+1)}(z)= \\left(- z + c\\int \\frac{t}{1+t \\tilde m^(l)(z)} \\nu(dt)   \\right)^{-1}.\n",
    "$$\n",
    "One should be careful since $m(z)$ is not defined for $z\\in \\text{supp} (\\mu)$. So one has to perform the algorithm for $z$ close to the real axis, that is $z=x+i\\varepsilon$, with $\\varepsilon \\ll 1$, (say $10^{-5}$), but the convergence can be quite slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f68d8b",
   "metadata": {},
   "source": [
    "Plot an approximation of the density of the distribution characterized by $m$ for the above example of $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "147e9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5eade",
   "metadata": {},
   "source": [
    "Compare with the histogram of the eigenvalues of $\\frac{1}{n} \\Sigma^{1/2} X X^\\intercal \\Sigma^{1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a48f94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7fe55",
   "metadata": {},
   "source": [
    "####  Population eigenvalue estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebf428",
   "metadata": {},
   "source": [
    "In view of the plot, it may be appear that averaging the sample eigenvalues of each component of the empirical spectral distribution provide a consistent estimator. But it is not the case, and such an estimator is indeed biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8888a",
   "metadata": {},
   "source": [
    "Visualization of the local behavior of the Stieltjes transform around an eigvanlue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220351da",
   "metadata": {},
   "source": [
    "Assume $p<n$. The Stieltjes transform of the sample covariance $n\\times n$ matrix $\\frac1n Y^\\intercal Y$, with $Y= \\Sigma^{1/2} X$ is\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{k=1}^p \\frac{1}{\\lambda_k-z} - \\frac{n-p}{n} \\frac{1}{z} \n",
    "$$\n",
    "where $\\lambda_1 \\leq \\cdots \\leq \\lambda_p$ are the sorted eigenvalues of $\\frac1n Y Y^{\\intercal}$ (and there are $n-p$ eigenvalues of $\\frac1n Y^\\intercal Y$ equal to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85ace1",
   "metadata": {},
   "source": [
    "It can be proved that the zeroes of the Stieltjes transform of the sample covariance $n\\times n$ matrix $\\frac1n Y^\\intercal Y$ are given by the eigenvalues of the following matrix:\n",
    "$$\n",
    "\\Lambda - \\frac{1}{n}\\sqrt \\lambda \\sqrt \\lambda^\\intercal,\n",
    "$$\n",
    "where $\\Lambda$ is the diagonal $p\\times p$ matrix $\\text{diag}(\\lambda_1,\\ldots,\\lambda_p)$, and $\\lambda$ the column vector $(\\lambda_1,\\ldots,\\lambda_p)$ in $\\mathbb R^p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c93c04",
   "metadata": {},
   "source": [
    "Plot the Stieltjes transform of the sample covariance matrix $\\frac1n  Y^\\intercal Y$, with $Y= \\Sigma^{1/2} X$ near one eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce1175ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e6b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "662aef17",
   "metadata": {},
   "source": [
    "Now, Mestre has proved that for $\\Sigma$ with eigenvalues $0<l_1<\\ldots<l_k$ with multiplicities $N_1,\\ldots,N_k$ (so $\\sum_{j=1}^k N_j = p$), the following gives a consistent estimator for $l_a$:\n",
    "$$\n",
    "\\hat l_a = \\frac{n}{N_a} \\sum_{j=N_1+\\cdots+N_{a-1}+1}^{N_1+\\cdots+N_{a}} \\left( \\lambda_j - \\rho_j  \\right),\n",
    "$$\n",
    "where $\\rho_1,\\ldots,\\rho_p$ are the zeroes of the Stieltjes transform of $\\frac1n Y^\\intercal Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4999b0a",
   "metadata": {},
   "source": [
    "For the above example for a population of 3 eigenvalues, compare the performance of the naive estimator of $l_a$ obtained by averaging the sample eigenvalues of each component of the empirical spectral distribution, and the above improved estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d98b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fd20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976069e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
