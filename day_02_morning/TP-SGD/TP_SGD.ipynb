{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668088ec-f3bb-4b7c-9aae-6d4dd210f967",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "In this tutiorial we will develop the basic SGD algorithm in the context of two common problems: a simple linear regression and logistic regression for binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "_**NB:** If you need to install a package in `Python` you may use the command `!pip install <package-name>` in a code cell._\n",
    "\n",
    "<small>**Credits:** This tutorial is based on [Joseph Boyd](https://jcboyd.github.io/assets/lsml2018/stochastic_gradient_descent.html) and [Francis Bach](https://www.di.ens.fr/~fbach/learning_theory_class_2024/index.html) tutorials.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0343-42b8-4fcf-aa8a-8f756bc1c701",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Before turning to the SGD algorithm (our focus today), let's take a look at the deterministic gradient decent algorithm on a very simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c49fd-109a-456e-bdd3-971556a30608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "import scipy.special as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205fd6c-4116-4fd5-9d31-b30f44e26b03",
   "metadata": {},
   "source": [
    "Consider the two functions $f$ and $g$ defined above\n",
    "\n",
    "##### <span style=\"color:purple\">**Todo:** Visualize these two functions.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c67c5-23cf-45a2-ab38-0c25ffcef094",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 2*x**4 + -2*x**3 -4*x**2 + 6\n",
    "g = lambda x: x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8022d2-1dd0-4ab0-97b4-26a6cf576557",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Plot graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18faaa8-4156-4147-b5a1-684282376a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/prelim/plot_graph.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93429861-a628-4d84-ae41-5f7b42f01f3c",
   "metadata": {},
   "source": [
    "The `grad_f` and `grad_g` functions defined below can be used to calculate the gradient of $f$ and $g$ respectively, at any point $x$.\n",
    "\n",
    "##### <span style=\"color:purple\">**Todo:** Given a gradient function (typically `grad_f` or `grad_g`), write a gradient descent algorithm.</span>\n",
    "\n",
    "The function will take as argument\n",
    "* `gradient` : Gradient function\n",
    "* `start` : Initial value for the GD\n",
    "* `learn_rate` : Learning rate for the GD\n",
    "* `n_iter` : Num of iterations\n",
    "* `vect` indicates whether the algorithm should also render the calculated values of theta (`vect=True`) or not (`vect=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7518a-590f-4584-87b5-05b4e618e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = lambda x: 4*2*x**3 + -2*3*x**2 -4*2*x\n",
    "grad_g = lambda x: 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e63d79-f938-4611-a43c-a4f8f2c007f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "\n",
    "def gradient_descent(gradient, start, learn_rate, n_iter, vect=False):\n",
    "    theta = ...\n",
    "    [...]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7985473-d212-4abe-a7ba-d53e3d599fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/prelim/gradient_descent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced9992-4707-41b4-abe2-9fff69d58500",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Go back to the figures above and visualize the gradient slope for $f$ and $g$ respectively.</span>\n",
    "\n",
    "You can vary the starting point, the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be944b-2f18-4429-b8af-929638010ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Influence of starting position, learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b6eb1-66f2-4a2e-95bc-38e2648227dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/prelim/gradient_descent_viz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1eed3-3294-4ddb-88fc-2df12b96b8a0",
   "metadata": {},
   "source": [
    "> Comments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44db740-14cf-4888-b11e-0a61c1def471",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "We are now going to move on to a so-called supervised statistical learning context: we have access to a labeled dataset $(x,y)$, and we are looking to make the link between observations $x$ and their label $y$.\n",
    "\n",
    "The aim of this section is to apply stochastic gradient descent to linear regression. Here, an observation corresponds to a pair $(X_i,y_i)$, where $X_i=(x_{i1},\\ldots,x_{ip})$ is the row matrix containing the $p$ measurements for experiment $i\\in\\{1,\\ldots,n\\}$, and $y_i$ is the label associated with these measurements. \n",
    "For example, we might want to explain the number of sales of a product as a function of the amount of advertising carried out for it (See Section 4).\n",
    "\n",
    "We start with a synthetic data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6b229-3ec0-4e9c-bc9e-b76d00591899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9453e1-b656-4043-bdcd-e3f55272f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, yy = make_regression(n_samples=100, \n",
    "                         n_features=1,\n",
    "                         n_informative=1,\n",
    "                         noise=20,\n",
    "                         random_state=0)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, xlabel='x', ylabel='y')\n",
    "ax.scatter(XX, yy, alpha=0.5)\n",
    "ax.set_title('Our dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26ce67-5e2e-4d95-a040-71b608c25912",
   "metadata": {},
   "source": [
    "This is typically a simple linear regression problem: For each sample $i\\in\\{1,\\ldots,n\\}$, we observe a couple $(x_i,y_i)\\in\\mathbb{R}^p\\times\\mathbb{R}$.\n",
    "\n",
    "##### <span style=\"color:purple\">**Question:** What is the dimention $p$ of the observations?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626a024-f297-4247-8e17-23a1aade0f88",
   "metadata": {},
   "source": [
    "> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473adecb-d8a5-4485-9995-d143d51343b8",
   "metadata": {},
   "source": [
    "Our aim is to learn a linear function $f_\\theta(x)=\\theta_0+\\theta_1x$ such that for all $i$, $y_i\\simeq f_\\theta(x_i)$. Note that this problem is equivalent to ask for $Y\\simeq X\\theta$, where $Y=(y_i)_i\\in\\mathbb{R}^n$, $X=(1,X_i)_i\\in\\mathcal{M}_{n,p+1}\\mathbb{R}$ and $\\theta=(\\theta_j)_j\\in\\mathbb{R}^p$. The matrix X thus constructed is called the _design matrix_.\n",
    "\n",
    "##### <span style=\"color:purple\">**Todo:** Construct matrix `X` and vector `y` from `XX` and `yy` previsously defined.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7a12d-ff26-4d1f-9e2f-968874d6606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Build X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3f5ab-1c17-4bd5-b2ea-a13b54fd9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/build_Xy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971582a2-f0ca-4fff-b4d6-54e336b6fe4a",
   "metadata": {},
   "source": [
    "### Least Square Estimator\n",
    "\n",
    "We will measure the learning error via the mean square error (MSE) loss:\n",
    "$$ \\mathcal{L}_{MSE}(\\theta) \\,=\\, \\frac1n\\Vert X\\theta-y\\Vert^2 \\,=\\, \\frac1n\\sum_{i=1}^n (\\theta_0+\\theta_1x_i-y_i)^2 $$\n",
    "Note that the gradient of $\\mathcal{L}_{MSE}$ is given by \n",
    "$$\\nabla\\mathcal{L}_{MSE}(\\theta)=\\frac2n X^\\top(X\\theta-y)$$\n",
    "\n",
    "##### <span style=\"color:purple\">**Todo:** Deduce a closed form for $\\theta^\\ast$, the minimum of this loss function.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975fe91-a0a5-416d-bfa7-5ae895b6c972",
   "metadata": {},
   "source": [
    "> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90f159-4afd-4af4-a8bb-0f810de42628",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** Does this computation seem reasonable to you when many different variables are observed, _i.e._ if $p$ is large?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1f796-83c2-4bad-b1c4-7b21665ff465",
   "metadata": {},
   "source": [
    "> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4764a5d-6121-4fc4-beec-d5416108ddae",
   "metadata": {},
   "source": [
    "The high-dimensional case for $\\theta$ is common in deep learning. In this experiment, we will assume that this is the context. However, for practical reasons, we will work in a low-dimensional setting (specifically, $p = 1$!). Still, the insights you gain here will remain valid in higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27437f68-3aa2-4bea-b7b4-97683b090ac7",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Calculate the optimal parameter vector $\\theta^\\ast$ associated with the formula you have just found. Make prediction $\\hat{y}=X\\theta^\\ast$ on training data using the learned parameters:<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2170061-1d36-48d3-b81a-35f653bf403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Optimal theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f804d0d-770b-40fa-a147-6a8affa0e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/optimal_theta.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4ccdc5-0590-4d9a-b2a3-6a5ea6b62ebf",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Visualize the regression line associated with `y_pred_star` on the `XX`, `yy` data set<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb9a8e-b431-4012-a0d1-0bbdb716ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Optimal theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45ee71-e7e8-4cdf-b63a-ec89a5f3cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/optimal_reglin.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17f52b-7613-42e9-95e4-a7adaf6db1db",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement the MSE loss<span>\n",
    "\n",
    "Compare the MSE value associated with $\\theta^\\ast$ to the noise used to generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d20d5-d8b9-4446-b7d9-3b517919ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "\n",
    "def mean_square_error(X, y, theta):\n",
    "    return ...\n",
    "\n",
    "mse = 0 ####\n",
    "print('MSE: %.02f RMSE: %.02f' % (mse, np.sqrt(mse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee135ed-7f7f-496f-a0d0-45f0bb02528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/mean_square_error.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d1d73-8e1a-4066-8e56-0a520137054c",
   "metadata": {},
   "source": [
    "> Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16d526-624d-4661-84d5-620a5efa1800",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "We recall taht the gradient of $\\mathcal{L}_{MSE}$ is given by \n",
    "$$\\nabla\\mathcal{L}_{MSE}(\\theta)=\\frac2n X^\\top(X\\theta-y)$$\n",
    "\n",
    "##### <span style=\"color:purple\">**Todo:** Implement a function to compute the gradient.<span>\n",
    "\n",
    "Check that this gradient is null in $\\theta^\\ast$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e4cfb-76b1-4862-b9d7-b492da7c76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "\n",
    "def grad_MSE(X, y, theta):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cbc85-192a-40bf-9957-295c95f883a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/grad_MSE.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dce15c-e67d-4927-beb9-0abab9948564",
   "metadata": {},
   "source": [
    "Recall that the stochastic gradient algorithm applies to a problem of the form\n",
    "$$ \\min_{\\theta\\in\\Theta}\\,\\mathbb{E}_{u\\sim\\mathbb{P}}[j(\\theta,u)] \\,. $$\n",
    "In this tutorial, we consider the MSE defined for the observed dataset $(X,y)$ by\n",
    "$$ MSE(\\theta) \\,=\\, \\frac1n\\Vert{X\\theta-y}\\Vert^2 \\,=\\, \\frac1n\\sum_{i=1}^n (X_i\\theta -y_i)^2 \\,. $$\n",
    "So, according to Monte Carlo approximation, we have\n",
    "$$ MSE(\\theta) \\,\\simeq\\, \\int_{u=(u_1,u_2)} (u_1\\theta-u_2)^2 \\mathrm{d}u \\,. $$\n",
    " <!-- \\mathbb{E}[(U_1\\theta-U_2)^2]$$ -->\n",
    "Now, we can applied the stochastic gradient descent! In fact, sampling $u=(u_1,u_2)\\in\\mathbb{R}^p\\times\\mathbb{R}$, where $u$ follows the same distribution as our original dataset, is trivially equivalent to pick-up a point in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838b22f-f772-4381-a9ff-24496a445fd1",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement the batch gradient descent algorithm for the linear regression problem.<span>\n",
    "\n",
    "You can draw inspiration from the code created in the preliminary section. Your function must take as input \n",
    "* `X` the design matrix for the MSE\n",
    "* `y` the label vector\n",
    "* `theta_init`\n",
    "* `learn_rate`\n",
    "* `n_iter`\n",
    "* `vect`, fedined as before\n",
    "  \n",
    "In particular, we do not give the gradient as an input value: we consider here that we are dealing exclusively with linear regression, and that we can therefore use the `grad_MSE` function without further precaution.\n",
    "\n",
    "We recall that the Batch version of the gradient descent correspond to a stocastic gradient descent where the Monte Carlo approximation is obtained from the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9269656-71cc-4a4e-bc05-35fdce7cd0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent_MSE(X, y, learn_rate=1e-1, n_iter=30, vect=False):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5277c-3db2-41e0-8c9c-7ff35e5a53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/batch_gradient_descent_MSE.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d44a9-dfb2-46d3-ba0b-3f81b73a1f19",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Visualize gradient descent during iterations.<span>\n",
    "\n",
    "Compare the theta obtained by GD with that obtained by exact calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44464d44-5060-4f21-b35b-9e0df5bc11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# GD visualization & theta comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487cc58-1dbd-49b1-bb87-3d2d1ec02834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/batch_gradient_descent_viz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf84958-f781-4a09-8a22-7b404f93d5ee",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Visualize the MSE loss function during itration for different learning rate.<span>\n",
    "\n",
    "A log scale on the y-axis can help to visualize the loss better if the learning rates vary \"a lot\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cefe4-671b-4b5c-80dc-85dbf04d1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Influence of the learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d735d-2a39-433d-92b1-aca03eab5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/batch_gradient_descent_lr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856138b0-b936-4ab9-9526-3b6c5719f030",
   "metadata": {},
   "source": [
    "> Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447d49a-0d9a-4fb8-871a-c2f58381d6c9",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Here, we have no particular difficulty in calculating the gradient using the whole observed dataset.\n",
    "However, _to practice_, we will code a \"pure\" stochastic version of this gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2c30c-457b-47a0-910d-c13738e96fd9",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement the stochastic gradient descent algorithm for the linear regression problem.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449397d5-0718-4b52-925e-b6b9dc28d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_MSE(X, y, theta_init, learn_rate=1e-2, n_iter=30, vect=False):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b207a4-cd58-49eb-8462-0bd7cf327a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/stochastic_gradient_descent_MSE.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d316a-f4e6-4812-9bd1-a9b494d85589",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Compare the batch grandient descent and the stochastic gradient descent.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fdcd31-1f55-478e-9035-94562fb3300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Compare gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a77d1c-1a37-4023-839c-a9a07ff0220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_lin/compare_gradient_descent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f681c3-21d0-4c5c-8003-b1a969c749a4",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "The. `Marketing_Data` dataset contains, for 200 experiments the advertising experiment between Social Media Budget and Sales (in\n",
    "Thousands $).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9a1ce-183e-43ab-b01e-6b4856fc0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Marketing_Data = pd.read_csv('Marketing_Data.csv')\n",
    "Marketing_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e17b6a-8c73-48dd-945c-a96a5d1b6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = Marketing_Data.iloc[:, :3].to_numpy()\n",
    "ones_column = np.ones((XX.shape[0], 1))\n",
    "\n",
    "X_sales = np.hstack((ones_column, XX))\n",
    "y_sales = Marketing_Data.iloc[:, 3].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0ac67-dd80-4b7b-9475-501894649bb5",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Question:** Can you predict the sales from the social media budget?<span>\n",
    "\n",
    "Answer this question numerically using a suitable algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a21a5-da17-44ad-8f4d-177456680a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e779941-2009-4ba8-972a-ff48b3345548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fbae3-3902-4770-95d3-c692f337f861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3b58c-1c0d-43b9-94da-b8fae7edeb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f102a0-9bf6-4445-9364-60373dd4fae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f775ddb-c57e-409b-8b7e-55fbe9c1eee8",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "For this problem, we will use the dataset available at [www.di.ens.fr/%7Efbach/orsay2017/data_orsay_2017.mat](http://www.di.ens.fr/%7Efbach/orsay2017/data_orsay_2017.mat)\n",
    "\n",
    "Although this is a Matlab file, we can use a scipy function to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65864b1-1657-492e-b375-314aed947b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d7cb8-1c31-4cd4-830b-e11555c01eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = io.loadmat('data_orsay_2017.mat')\n",
    "\n",
    "XX_train, y_train = data['Xtrain'], data['ytrain']\n",
    "XX_test, y_test = data['Xtest'], data['ytest']\n",
    "\n",
    "print('XX_train shape: %s' % str(XX_train.shape))\n",
    "print('y_train shape: %s' % str(y_train.shape))\n",
    "print('XX_test shape: %s' % str(XX_test.shape))\n",
    "print('y_test shape: %s' % str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc4b26-8533-4b8b-9679-a81eb08ab1a9",
   "metadata": {},
   "source": [
    "In logistic regression, we encode the positive and negative classes as $y\\in\\{-1,1\\}$. \n",
    "Thus, \n",
    "$$ p(y=1\\mid X\\theta) = \\sigma(X\\theta) \\qquad\\text{and}\\qquad p(y=-1 \\mid X\\theta) = 1-\\sigma(X\\theta) = \\sigma(-X\\theta) \\,, $$\n",
    "where $\\displaystyle\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ is the sigmoid function, and $X$ is the design matrix (_i.e._ the observation matrix completed by a column of $1$).\n",
    "\n",
    "To train the model, we minimize the _binary cross-entropy_ between the predicted distribution and the ground truth, which we can assume to be _one-hot_, assigning all probability to the correct class. This simplifies the loss function to\n",
    "$$ \\mathcal{L}_{BCE}(\\theta) \\,=\\, \\frac1n\\sum_{i=1}^n \\log\\big(1+\\exp(-y_i\\,X_i\\theta)\\big) \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ccff4-43a1-4004-87b4-d861e78f24b3",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement the loss function of the logistic regression, _i.e._ the binary cross entropy.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbdb36-c3f1-49e2-ab98-2536f1a5558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(X, y, theta):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12125e82-1fcc-4340-bba4-237fc7b89ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_log/binary_cross_entropy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece478c0-2ea6-4d6e-bb81-88aedaf25ff9",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement the gradient of the previous loss.<span>\n",
    "\n",
    "Note that \n",
    "$$ \\nabla\\mathcal{L}_{BCE}(\\theta) \\,=\\, \\sum_{i=1}^n \\frac{-y_i\\,X_i}{1+\\exp(y_i\\,X_i\\theta)} \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df743e-a0f6-4b61-8e64-5442924fc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_BCE(X, y, theta):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba23ae7-ff5b-4fac-b1f6-2e9be7281e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_log/grad_BCE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061ccd8-05db-4eaf-91c0-c4cf8f541305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design matrices\n",
    "\n",
    "X_train = np.concatenate([np.ones((XX_train.shape[0], 1)), XX_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((XX_test.shape[0], 1)), XX_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8bcd4e-99b1-4c44-a609-2c17612016b4",
   "metadata": {},
   "source": [
    "### Mini-Batch Stochastic Gradient Descent\n",
    "\n",
    "In large-scale applications, computing the full gradient can be computationally expensive. Moreover, using a small sample of size $m<n$ from a large dataset at each iteration is often sufficient for making an accurate descent step. Minibatch gradient descent addresses this by using a subsample of size $m$ at each iteration. In the extreme case where $m= 1$, this method is known as stochastic gradient descent (SGD). As a result, the complexity of the gradient computation is reduced from $\\mathcal{O}(np)$ to $\\mathcal{O}(mp)$.\n",
    "\n",
    "In the context of training deep neural networks, minibatch gradient descent (and its variants) has become the most widely used approach. Additionally, the inherent stochasticity of this method can help avoid getting stuck in local minima of non-convex loss functions.\n",
    "\n",
    "Operationally, the main change is the size of the data fed into the gradient function, referred to as the ``batch''. The most straightforward strategy for selecting a batch is to cycle through the (pre-shuffled) dataset and slice the next $m$ values. \n",
    "A full cycle of the training data is known as an _epoch_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ecbafe-6fc5-473c-9e12-fc8fdcc8a248",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Implement a cycling strategy for `minibatch_gradient_descent` with a given `batch_size`.<span>\n",
    "\n",
    "Follow the same structure as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77956b-3c08-41ed-ac07-3c251e6d3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_minibatch_gradient_descent_BCE(X, y, theta_init, learn_rate=1e-2, n_iter=30, vect=False):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b690bb-bf35-4344-bab9-2a9218125a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_log/cycle_minibatch_gradient_descent_BCE.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28637b-dfa9-4b73-a8c2-f02c6940da94",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Evaluate the accuracy of the estimate using the test set.<span>\n",
    "\n",
    "1. Estimate a parameter $\\theta$ using `cycle_minibatch_gradient_descent_BCE`\n",
    "2. Using the [`expit`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html) function in the `scipy.special` package, compute the output probabilities\n",
    "3. Make a prediction by thresholding the probabilities at $0.5$: any probabilty $\\geqslant0.5$ we will say is positive; any $<0.5$ negative\n",
    "4. Check model accuracy using the [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function from `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4b4de-8795-4d74-8cbe-9ee0ec467e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a473f-0f5d-44fc-a33a-e918f711c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Estimation accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f6519f-b1a2-45bf-94b7-12f6a0cd01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = np.random.rand(X_train.shape[1],1)\n",
    "batch_size = 20\n",
    "learn_rate=1e-2\n",
    "n_iter=30\n",
    "\n",
    "theta_cycle_MSE = cycle_minibatch_gradient_descent_BCE(X_train, y_train, theta_init, batch_size,  learn_rate, n_iter, vect=False)\n",
    "probs = sp.expit(X_test.dot(theta_cycle_MSE))\n",
    "y_pred_cycle_BCE = np.where(probs >= 0.5, 1, -1)\n",
    " \n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_cycle_BCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba4571-6965-4d32-933b-f5af12756442",
   "metadata": {},
   "source": [
    "> Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542d8d5-96d9-40d1-8153-1c79d70f5c52",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** visualise the descent curves for different batch size.<span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45157b93-5211-4bf1-b50a-2d02652c5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Batch size comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636ec75-2d8e-41cf-b248-610d38bad304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_log/cycle_minibatch_gradient_descent_viz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0be9cf-e7ac-488f-8070-c30a94110dc7",
   "metadata": {},
   "source": [
    "An alternative to our cycling strategy is to randomly sample a batch at each iteration (like we did for the logistique regression).\n",
    "\n",
    "##### <span style=\"color:purple\">**Todo:** Implement a sampling strategy for `minibatch_gradient_descent` with a given `batch_size`.<span>\n",
    "\n",
    "Compare both method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f065f2-7ce5-40a1-bdb5-2ac0ef1ac18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_minibatch_gradient_descent_BCE(X, y, theta_init, batch_size, learn_rate=1e-2, n_iter=30, vect=False):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c217afb-0227-4540-b70a-e6199d853317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/reg_log/cycle_minibatch_gradient_descent_BCE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830445ec-1071-47a8-8624-00951f868934",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# Comparison of the methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939b711-4c22-49f2-8949-6eae26e13422",
   "metadata": {},
   "source": [
    "> Comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4d930-6020-4251-9bd8-03d3699e7f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d25e28-f29a-401a-8567-32215172bb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa065b-d833-446d-8923-bd41302ecc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4MA-AD",
   "language": "python",
   "name": "4ma-ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
