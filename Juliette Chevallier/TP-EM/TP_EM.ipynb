{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668088ec-f3bb-4b7c-9aae-6d4dd210f967",
   "metadata": {},
   "source": [
    "# Expectation-Maximization for the Gaussian Mixture Model\n",
    "\n",
    "---\n",
    "\n",
    "In this tutiorial we will develop the Expectation-Maximization (EM) algorithm, and some of its statistic variants, in the context of Gaussian Mixture Model (GMM).\n",
    "\n",
    "---\n",
    "\n",
    "_**NB:** If you need to install a package in `Python` you may use the command `!pip install <package-name>` in a **code** cell._\n",
    "\n",
    "\n",
    "<small>**Credits:** This tutorial is based on [zfeng](https://medium.com/@zhe.feng0018/coding-gaussian-mixture-model-and-em-algorithm-from-scratch-f3ef384a16ad)'s' one.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d0343-42b8-4fcf-aa8a-8f756bc1c701",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "A GMM is useful for modeling data that comes from one of several groups: the groups might be different from each other, but data points within the same group can be well-modeled by a Gaussian distribution. The main issue is to estimate the parameters of the mixture, _i.e_ to find the most likely ones. Moreover, we aim to determine if our sample follow a Gaussian mixture distribution or not.\n",
    "\n",
    "Let consider a $n$-sample $Y=(Y_1,\\,\\ldots,\\,Y_n)$ of $\\mathbb{R}^d$. For each individual, we observe a random variable $Y_i$ and assume there is an unobserved variable $Z_i$ for each person which encode the class of $Y_i$. More formally, we consider a mixture of $K$ Gaussian: For all $i\\in\\{1,\\ldots,n\\}$ and $k\\in\\{1,\\ldots,K\\}$,\n",
    "$$ \n",
    "\\left\\{\\begin{aligned}\n",
    "\t& Z_i \\,;\\, \\theta ~\\sim~ \\sum_{k=1}^K \\alpha_k\\, \\delta_k \\,, \\\\\n",
    "\t& Y_i ~\\vert~ \\{Z_i=k\\} \\,;\\, \\theta ~\\sim~ \\mathcal{N}(\\mu_k,\\Sigma_k) \\,,\n",
    "\\end{aligned}\\right.\n",
    "$$\n",
    "where\n",
    "* $\\alpha=(\\alpha_1,\\ldots,\\alpha_K)\\in\\mathbb{R}_+^K$ checks $\\sum_{k=1}^K\\alpha_k=1$ and denotes the _mixing distribution_,\n",
    "* $\\mu=(\\mu_1,\\ldots,\\mu_K)\\in\\mathbb{R}^{dK}$ denotes the set of _means_, and\n",
    "* $\\Sigma=(\\Sigma_1,\\ldots,\\Sigma_K)\\in(\\mathcal{S}_d^+)^{K}$ denotes the set of _covariance matrices_.\n",
    "\n",
    "Unless otherwise stated, we suppose that $K$ is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "410c49fd-109a-456e-bdd3-971556a30608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "#import scipy.special as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fa669-e669-4448-b522-adfd7bbe544a",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "##### <span style=\"color:purple\">**Question:** What are the paramters of the model ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215ed41-8fe3-4526-b339-5ac93e4bac73",
   "metadata": {},
   "source": [
    "> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c91fb-d1b0-4e13-9a98-b444738d542e",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write down the likelihood of the model.</span>\n",
    "\n",
    "Based on the two `gaussian_pdf` and `component_pdfs` functions provided, write a `likelihood_function` that calculates the likelihood for this mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151133d7-f55a-4d7a-b8ae-09df309afc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    # x vector should be in column, but numpy treat 1-d array as vector so can save the extra step\n",
    "    n = len(x)\n",
    "    return (2*np.pi)**(-n/2) * (la.det(sigma) ** (-1/2) ) * np.exp(-1/2*((x-mu).T@la.inv(sigma)@(x-mu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22706e84-d5e0-4077-9a06-e5493bc0cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_pdfs(x, mus, sigmas):\n",
    "    \"\"\"\n",
    "    The component pdf p_k(x; mu_k, sigma_k)\n",
    "        :param mus: Kxn array (n be the dimension of x vector and K be the num of components), mean of k n-dim gaussian distr.\n",
    "        :param sigmas: Knxn array covariance of k n-dim gaussian distr.\n",
    "        :return: K-dim array contain probability of each component pdf\n",
    "    \"\"\"\n",
    "    n_components = mus.shape[0]\n",
    "    return np.array([gaussian_pdf(x, mus[k,:], sigmas[k, :, :]) for k in range(n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fafb988-a898-412d-9675-f7bc8242715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "\n",
    "def likelihood_function(X, taus, mus, sigmas):\n",
    "    \"\"\"\n",
    "    The component pdf p_k(x; mu_k, sigma_k)\n",
    "        :param taus: K-dim array contains the weight (or prior of hidden var) of each gaussian component\n",
    "        :param mus: Kxn array (n be the dimension of x vector and K be the num of components), mean of k n-dim gaussian distr.\n",
    "        :param sigmas: Knxn array covariance of k n-dim gaussian distr.\n",
    "        :return: numeric between 0 and 1, the likelihood function value\n",
    "    \"\"\"\n",
    "\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "991d53ae-a685-4fbe-8ccc-d3d0929ab831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/likelihood_function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4c432-d77b-4022-8080-baa9a38a4175",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">✍️ **Todo:** Write down the complete log-likelihood of the model.</span>\n",
    "\n",
    "Prove that le complete log-likelihood of the  model writes: For all $y\\in\\mathbb{R}^d$ and $z\\in\\{1,\\ldots,K\\}$, \n",
    "$$\n",
    "\\log q(y,\\,z\\,;\\,\\theta) \n",
    "    \\,=\\, \\sum_{i=1}^n \\sum_{k=1}^K \\log\\left( f_{\\mathcal{N}(\\mu_k,\\sigma_k)}(y_i) \\right) \\mathbb{1}_{\\{z_i=k\\}}\n",
    "    \\,+\\, \\sum_{i=1}^n \\sum_{k=1}^K \\log(\\alpha_k) \\mathbb{1}_{\\{z_i=k\\}} \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd21de-cf94-4195-90ea-0e6757a357b3",
   "metadata": {},
   "source": [
    "> **Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da271b-3056-4e09-b5c0-41908b1b4e56",
   "metadata": {},
   "source": [
    "## E-step\n",
    "\n",
    "##### <span style=\"color:purple\">✍️ **Todo:** Compute the conditional expectation</span>\n",
    "$$\n",
    "Q(\\theta \\,\\vert\\, \\theta^{(t)}) \\,=\\, \\mathbb{E}_{Z\\,\\sim\\,p(\\cdot\\vert y\\,;\\,\\theta^{(t)})} \\left[\\, \\log q(y,\\,z\\,;\\,\\theta) \\,\\vert\\, y \\,;\\, \\theta^{(t)} \\,\\right]\n",
    "$$\n",
    "\n",
    "1.  Justify that the $E$-step amounts to compute $\\tau_{i,k} = \\tau_{i,k}^{(t)} := \\mathbb{P}\\left(Z_i=k \\,\\vert\\, y_i\\,;\\,\\theta^{(t)}\\right)$ for all $i$ and $k$.\n",
    "2.  Using the Bayes rule, writes the $(\\tau_{i,k})_{i\\in\\{1,\\ldots,n\\},\\, i\\in\\{1,\\ldots,K\\}}$ in a closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34032b0b-15b5-4e24-abc8-0302332a72cb",
   "metadata": {},
   "source": [
    "> **Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e26cad-db16-4af8-b29b-5525af1f844b",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write an 'e_step' function to perform the E-step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2791aa2-fe0f-49b1-8d04-c5ff1d527748",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "\n",
    "def e_step(X, taus, mus, sigmas):\n",
    "    \"\"\"\n",
    "        E step of the EM algorithm, caculates the posterior T_{k, i}=P(z_i=k|y_i)\n",
    "        it returns T_{k,i} in the form of a KxN T matrix where each element is T_{k, i}\n",
    "        :param X: Nxn matrix represents N number of n-dim data points\n",
    "        :param taus: K-dim vector, the weight of each component, or the prior of the hidden stats z\n",
    "        :param mus: Kxn matrix (n be the dimension of x vector and K be the num of components), mean of k n-dim gaussian distr.\n",
    "        :param sigmas: Kxnxn matrix covariance of k n-dim gaussian distr.\n",
    "        :return: T_{k,i} in the form of a KxN T matrix where each element is T_{k, i}\n",
    "    \"\"\"\n",
    "\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f4fd25-af30-48bf-9c71-b53a5e9a7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/e_step.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66aa89f-035d-4488-8de5-b3271941992a",
   "metadata": {},
   "source": [
    "## M-step\n",
    "\n",
    "##### <span style=\"color:purple\">✍️ **Todo:** Maximize $Q(\\,\\cdot\\,\\vert\\, \\theta^{(t)})$ under the constraint that $\\sum_{k=1}^K\\alpha_k=1$</span>\n",
    "\n",
    "1. For all $k\\in\\{1,\\ldots,K\\}$, find the maximum in $\\alpha_k$ of $Q(\\,\\cdot\\,\\vert\\, \\theta^{(t)})$.\n",
    "\t\t\n",
    "2. For all $k\\in\\{1,\\ldots,K\\}$, find the maximum in $\\mu_k$ of $Q(\\,\\cdot\\,\\vert\\, \\theta^{(t)})$.\n",
    "\n",
    "_Recall that for any symmetric matrix $A\\in\\mathcal{S}_d^+$ and any vector $u\\in\\mathbb{R}^d$, the gradient of ${u^\\top}Au$ with respect to $u$ is $2Au$._\n",
    "\n",
    "3. For all $k\\in\\{1,\\ldots,K\\}$, find the maximum in $\\Sigma_k$ of $Q(\\,\\cdot\\,\\vert\\, \\theta^{(t)})$. \n",
    "\n",
    "_Recall that for any positive symmetric matrix $A\\in\\mathcal{S}_d^+$, the gradient of $\\log\\vert A\\vert$ with respect to $A$ is $A^{-1}$. Moreover, for any vectors $u,v\\in\\mathbb{R}^d$ the gradient of ${u^\\top}Av$ with respect to $A$ is the matrix $u{v^\\top}$._\n",
    "\t\t\n",
    "4. Interpret those quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10c511-0335-4923-ae58-b1df03d375bd",
   "metadata": {},
   "source": [
    "> **Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e294378-5690-48f9-8822-f5a9b97944fc",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Write a 'm_step' function to perform the M-step.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46570283-6fcd-47ac-bfe6-8b764cc95c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "\n",
    "def m_step(X, T):\n",
    "    \"\"\"\n",
    "        M step of the EM algorithm, caculates the MLE of taus, mus and sigmas\n",
    "        :param X: Nxn matrix, the dataset, N number of n-dim data points\n",
    "        :param T: KxN matrix, the T matrix is the posterior matrix where the i, j th component is the T_{k, i}\n",
    "        :return: a 3-tuple:\n",
    "            - taus: K-dim array, the estimated prior probability for each hidden variable z\n",
    "            - mus: Kxn matrix, the estimated mean of the n-dim gaussian component, for each of the k component\n",
    "            - sigmas: Kxnxn matrix, the covariance matrix of the n-dim gaussian component, for each of the k component\n",
    "    \"\"\"\n",
    "\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f4cb2c-476c-4809-b4a0-984c5eccd7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/m_step.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776d06f-ee64-4389-af6d-fad3420575e6",
   "metadata": {},
   "source": [
    "Given you codes, an EM-training loop writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05279c4b-79ce-4e28-952a-ce7ad8aa8ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "for i in range(100):\n",
    "    T = e_step(X, taus, mus, sigmas)\n",
    "    sigmas_prev = sigmas\n",
    "    taus, mus, sigmas = m_step(X, T)\n",
    "    if np.min(abs(sigmas - sigmas_prev) < 0.1):\n",
    "        print(f\"break after {i}th iteration\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720802b6-3bc1-4811-8821-a562a69e017e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf59e8-3edc-4521-a22f-fa859f676566",
   "metadata": {},
   "source": [
    "We put all this function in a \"big\" class for a more convenient use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f4e3a73-5b31-4f79-a454-10f7b2357706",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture():\n",
    "    def __init__(self, n_hidden=2, max_iter=100, seed=None):\n",
    "        \"\"\"\n",
    "            :param n_hidden: number of hidden variables (or the number of components)\n",
    "            :param max_iter: maximum EM iteration allowed, default to 100\n",
    "            :param seed: the random seed for initialisation\n",
    "        \"\"\"\n",
    "        self.n_hidden = n_hidden\n",
    "        self.max_iter = max_iter\n",
    "        self.seed = seed\n",
    "        self.taus, self.mus, self.sigmas = None, None, None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "            :param X: Nxn matrix (N be the num of data points and n be the dimension of each data point)\n",
    "        \"\"\"\n",
    "        n_hid = self.n_hidden\n",
    "        n_var = X.shape[1]\n",
    "\n",
    "        np.random.seed(self.seed) # setup seed, if None means no seed\n",
    "        mus = np.random.randn(n_hid, n_var)*10 # initialise means of k components\n",
    "        np.random.seed(self.seed)\n",
    "        # initialise  sigmas of k components with identity\n",
    "        sigmas = np.array([np.eye(n_var) for _ in range(n_hid)])\n",
    "        taus = np.ones(n_hid)/n_hid # assume uninformative prior\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            T = e_step(X, taus, mus, sigmas)\n",
    "            sigmas_prev = sigmas\n",
    "            taus, mus, sigmas = m_step(X, T)\n",
    "            if np.min(abs(sigmas - sigmas_prev) < 0.1):\n",
    "                print(f\"break after iteration {i+1}\")\n",
    "                break\n",
    "        self.taus, self.mus, self.sigmas = taus, mus, sigmas\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        T = e_step(X, self.taus, self.mus, self.sigmas)\n",
    "        return T.T # transpose, so it's 1st dimension matches the X's dimension N.\n",
    "\n",
    "    def predict(self, X):\n",
    "        T = e_step(X, self.taus, self.mus, self.sigmas)\n",
    "        return np.argmax(T.T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69592904-8c30-44ed-bf4a-82342067dfdb",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Comment on the `predict_proba` and `predict` functions.</span>\n",
    "\n",
    "What can they achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2f5dc-9180-4d57-8845-211b04bb4a9f",
   "metadata": {},
   "source": [
    "> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500db7e-c84b-44df-93f7-6b2d4f0be04d",
   "metadata": {},
   "source": [
    "##### <span style=\"color:purple\">**Todo:** Test your algorithm</span>\n",
    "\n",
    "1. Generate a dataset\n",
    "2. Fit a Gaussian Mixture to your data\n",
    "3. Discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4250de79-db39-4201-9e2b-c419c31e9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO BE COMPLETED ##\n",
    "# > Test the model <\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab42f8-5efa-4f8f-938f-f1dd9516cb59",
   "metadata": {},
   "source": [
    "## SAEM Algorithm\n",
    "\n",
    "##### <span style=\"color:purple\">✍️ **Todo:** Justify that the Gaussian mixture model belongs to the exponential family</span>\n",
    "\n",
    "1. Justify that the Gaussian mixture model belongs to the exponential family\n",
    "2. Provide sufficient statistics\n",
    "3. What happens to the M-step in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bcae7-0593-4d32-a590-cf8e365f29de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc37f28-66b2-4463-9916-5377bbada690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4MA-AD",
   "language": "python",
   "name": "4ma-ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
